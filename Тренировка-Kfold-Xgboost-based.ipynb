{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import gc\n",
    "import operator\n",
    "import pickle\n",
    "from copy import deepcopy\n",
    "from statistics import mean\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from xgboost import (XGBClassifier)\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedGroupKFold,StratifiedKFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "import gensim\n",
    "import gensim.models as g\n",
    "import gensim.downloader\n",
    "\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.en import English\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from transformers import BertTokenizer, TFBertModel, BertModel, AutoTokenizer, AutoModel, AutoModelForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from datasets import load_metric\n",
    "import torch\n",
    "\n",
    "\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "\n",
    "# Манипулирование данными\n",
    "import pandas as pd # для манипулирования данными\n",
    "\n",
    "# Визуализация\n",
    "import plotly.express as px # для визуализации данных\n",
    "import matplotlib.pyplot as plt # для отображения рукописных цифр"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_all_files_texts(prefix_file='./CO_fold/'):\n",
    "    \"\"\"\n",
    "    Закачка документов в буфер памяти\n",
    "    \"\"\"\n",
    "    file_names = os.listdir(prefix_file)\n",
    "    text_file_list = []\n",
    "    annotation_file_list = []\n",
    "    for i in file_names:\n",
    "        if '.txt' == i[-4:]:\n",
    "            with open(prefix_file+i) as file:\n",
    "                 text_file_list.append(file.read())\n",
    "            with open(prefix_file+i[:-4]+'.ann') as file:\n",
    "                 annotation_file_list.append(file.read())\n",
    "    return text_file_list, annotation_file_list\n",
    "def parse_ann_file(text):\n",
    "    \"\"\"\n",
    "    Парсинг файлов анотации\n",
    "    \"\"\"\n",
    "    text = list(filter(lambda x: x!='', text.split('\\n')))\n",
    "    text = [i.split('\\t') for i in text]\n",
    "    text = [[i[1].split(), i[2]] for i in text]\n",
    "    text = [{'name': i[0][0], 'start': int(i[0][1]), 'end': int(i[0][-1]), 'text': i[1]} for i in text]\n",
    "    return text \n",
    "def highlight_description(ann_text, raw_text):\n",
    "    \"\"\"\n",
    "    Выделение текста операторскими скобками\n",
    "    \"\"\"\n",
    "    filtered_list = list(filter(lambda x: x['name']=='Description', ann_text))\n",
    "    filtered_list = sorted(filtered_list, key=lambda x: x['end'], reverse=True) \n",
    "    for i in filtered_list:\n",
    "        if raw_text[i['end']]=='.':\n",
    "            raw_text = raw_text[:i['end']]+'{__enddesc__}'+raw_text[i['end']:]\n",
    "        else:\n",
    "            k = 1\n",
    "            while raw_text[i['end'] - k]==' ' or raw_text[i['end'] - k]=='\\t':\n",
    "                k += 1\n",
    "            raw_text = raw_text[:i['end']-1]+'{__enddesc__}'+raw_text[i['end']-1:]\n",
    "        raw_text = raw_text[:i['start']-1]+'{__startdesc__}'+raw_text[i['start']:]\n",
    "    return raw_text\n",
    "def replace_all(text, dic):\n",
    "    \"\"\"\n",
    "    Множественная замена в строке\n",
    "    \"\"\"\n",
    "    for i, j in dic.items():\n",
    "        text = text.replace(i, j)\n",
    "    return text\n",
    "def soft_clean(text):\n",
    "    \"\"\"\n",
    "    очистка текста от операторов, которые не влияют на предложения\n",
    "    \"\"\"\n",
    "    reg = r'(\\\\)(\\w+)(=\\w+|\\W*|(\\[\\w+\\])+(\\{\\w+\\})+|(\\{\\w+\\}+)+)\\s*\\n*|((\\\\)(\\w+))+\\s*\\n|(\\%+\\s*\\n)'\n",
    "    dict_for_repl = {'a.k.a.': 'aka', 'e.g.':'eg', 'resp.':'resp', 't.s.':'ts', 't.i.': 'ti'}\n",
    "    cleaned = text.split('\\n')\n",
    "    cleaned = list(filter(lambda x: not re.fullmatch(reg, x), cleaned))\n",
    "    return replace_all(' '.join(cleaned).lower(), dict_for_repl)\n",
    "def start_of_text(raw_text, start_of_raw):\n",
    "    \"\"\"\n",
    "    возвращает текст с начала содержания статьи\n",
    "    \"\"\"\n",
    "    return raw_text[raw_text.find(start_of_raw):]\n",
    "def add_clean(text):\n",
    "    \"\"\"\n",
    "    дополнительная очистка и подготовка датасета\n",
    "    \"\"\"\n",
    "    dict_for_repl = {'}_{':'_', '\\\\{':' ', '\\\\}':' ', '{':' ', '}':' ', '$': ' $ ', ' \\\\ ':' ', '\\[':' \\[ ', '\\]':' \\] '}\n",
    "    return re.sub(' +', ' ', replace_all(text, dict_for_repl))\n",
    "def hamming_distance(list1,list2):\n",
    "    \"\"\"\n",
    "    Просмотр ошибок через расстояние Хэмминга\n",
    "    \"\"\"\n",
    "    result =0\n",
    "    for x,(i,j) in enumerate(zip(list(list1),list2)):\n",
    "        if i!=j:\n",
    "            print(f'char not math{i,j}in {x}')\n",
    "            result+=1\n",
    "    print(f\"Расстояние Хэмминга = {result}\")\n",
    "def vectorize_sentence(sentence,model):\n",
    "    \"\"\"\n",
    "    Модель для работы с векторными представляниями GloVe и Word2Vec\n",
    "    \"\"\"\n",
    "    nlp = English()\n",
    "    tokenizer = Tokenizer(nlp.vocab)\n",
    "    a = []\n",
    "    for i in tokenizer(sentence):\n",
    "        try:\n",
    "            a.append(model.get_vector(str(i)))\n",
    "        except:\n",
    "            pass\n",
    "    a=np.array(a).mean(axis=0)\n",
    "    a = np.zeros(300) if np.all(a!=a) else a\n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обработка статей\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# вывод списка статей\n",
    "print(os.listdir('./CO_fold'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# считывание файлов датасета\n",
    "text_file_list, annotation_file_list = return_all_files_texts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# символы начала статей\n",
    "start_of_raw_list = [\"{__startdesc__}Given a positive integer $n \\in \\mathbb{Z}_{+}$\",\n",
    "\"The purpose of this paper is to give a class of reconstructible graphs.\",\n",
    "\"{__startDesc__}Let $B$ be properly $n$- colored bipartite multigraph with $n+1$ edges of each color{__endDesc__}. \",\n",
    "\"Exercise VII.47 of [PS] (brought to my attention by Richard Stanley),\",\n",
    "\"Let $R=\",\n",
    "\"Let $G$ be a simple graph.{__startDesc__}The collection $D(G)=(G_v)_{v\\in V(G)}$\",\n",
    "\"{__startDesc__}A\",\n",
    "\"{__startDesc__}Stern's diatomic sequence $a_1=1,  a_{2n}=a_n,  a_{2n+1}=a_n+a_{n+1}$\",\n",
    "\"We prove a classification theorem for Hankel weighing matrices.\",\n",
    "\"This billet should be regarded as a footnote to \\cite{GL}\",\n",
    "\"Since  very few papers  concern  maximal symplectic partial spreads in dimension $>4$\",\n",
    "\"{__startDesc__}The Rogers-Ramanujan identities\"]\n",
    "start_of_raw_list = [i.lower() for i in start_of_raw_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# обработка предложений\n",
    "for i in range(len(text_file_list)):\n",
    "    raw = text_file_list[i]\n",
    "    ann = parse_ann_file(annotation_file_list[i])\n",
    "    raw = highlight_description(ann, raw)\n",
    "    raw = soft_clean(raw)\n",
    "    text_file_list[i] = start_of_text(raw, start_of_raw_list[i])\n",
    "    text_file_list[i] = text_file_list[i].split('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создание списков меток класса\n",
    "all_sent = []\n",
    "labels = []\n",
    "for i in text_file_list:\n",
    "    for j in i:\n",
    "        if j.find('{__startdesc__}')!=-1  and j.find('{__enddesc__}')!=-1:\n",
    "            all_sent.append(j.replace('{__startdesc__}', '').replace('{__enddesc__}', ''))\n",
    "            labels.append(1)\n",
    "        else:\n",
    "            all_sent.append(j)\n",
    "            labels.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# дополнительно очищаем датасет\n",
    "all_sent = [add_clean(i) for i in all_sent]\n",
    "# закрываем операторские скобки\n",
    "for i in range(len(all_sent)):\n",
    "    if all_sent[i].count('$')%2 == 1:\n",
    "        all_sent[i] += '$'\n",
    "        all_sent[i+1] = '$' + all_sent[i+1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# вывод количества примеров, количество положительных меток, пример меток\n",
    "print(labels.__len__())\n",
    "print(sum(labels))\n",
    "print(labels[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del text_file_list, annotation_file_list\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#загрузка дополнительных данных для обучения tf-idf и bow\n",
    "add_list_for_stat, _ =   return_all_files_texts('../download/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(add_list_for_stat.__len__()):\n",
    "    raw = add_list_for_stat[i]\n",
    "    raw = soft_clean(raw)\n",
    "    add_list_for_stat[i] = raw.split('.')\n",
    "extra_sentence = [add_clean(j) for i in add_list_for_stat for j in i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_sent = extra_sentence + all_sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Подборка векторного представления"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Fold with optimizing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial: optuna.Trial) -> float:\n",
    "    global X, y, samples, sample_labels\n",
    "    param = {\n",
    "        \"verbosity\": 0,\n",
    "        \"objective\": trial.suggest_categorical(\"objective\",[\"binary:logistic\",\"binary:logitraw\",\"binary:hinge\"]),\n",
    "        # use exact for small dataset.\n",
    "        \"tree_method\": \"exact\",\n",
    "        # defines booster, gblinear for linear functions.\n",
    "        \"booster\": trial.suggest_categorical(\"booster\", [\"gbtree\", \"gblinear\", \"dart\"]),\n",
    "        # L2 regularization weight.\n",
    "        \"lambda\": trial.suggest_float(\"lambda\", 1e-8, 1.0, log=True),\n",
    "        # L1 regularization weight.\n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 1e-8, 1.0, log=True),\n",
    "        # sampling ratio for training data.\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.2, 1.0),\n",
    "        # sampling according to each tree.\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.2, 1.0),\n",
    "        \"scale_pos_weight\": 50.8311688311688\n",
    "    }\n",
    "\n",
    "    if param[\"booster\"] in [\"gbtree\", \"dart\"]:\n",
    "        # maximum depth of the tree, signifies complexity of the tree.\n",
    "        param[\"max_depth\"] = trial.suggest_int(\"max_depth\", 3, 30, step=2)\n",
    "        # minimum child weight, larger the term more conservative the tree.\n",
    "        param[\"min_child_weight\"] = trial.suggest_int(\"min_child_weight\", 2, 10)\n",
    "        param[\"eta\"] = trial.suggest_float(\"eta\", 1e-8, 1.0, log=True)\n",
    "        # defines how selective algorithm is.\n",
    "        param[\"gamma\"] = trial.suggest_float(\"gamma\", 1e-8, 1.0, log=True)\n",
    "        param[\"grow_policy\"] = trial.suggest_categorical(\"grow_policy\", [\"depthwise\", \"lossguide\"])\n",
    "\n",
    "    if param[\"booster\"] == \"dart\":\n",
    "        param[\"sample_type\"] = trial.suggest_categorical(\"sample_type\", [\"uniform\", \"weighted\"])\n",
    "        param[\"normalize_type\"] = trial.suggest_categorical(\"normalize_type\", [\"tree\", \"forest\"])\n",
    "        param[\"rate_drop\"] = trial.suggest_float(\"rate_drop\", 1e-8, 1.0, log=True)\n",
    "        param[\"skip_drop\"] = trial.suggest_float(\"skip_drop\", 1e-8, 1.0, log=True)\n",
    "\n",
    "    skf = StratifiedKFold(5, shuffle=True, random_state=33)\n",
    "    f1_list = []\n",
    "    acc_list = []\n",
    "    for train_index, test_index in skf.split(X,y):\n",
    "        gbm = XGBClassifier(**param)\n",
    "        gbm.fit(\n",
    "            X[train_index],\n",
    "            y[train_index],\n",
    "            verbose=0,\n",
    "        )\n",
    "        preds = gbm.predict(X[test_index])\n",
    "        pred_labels = np.rint(preds)\n",
    "        f1_list.append(f1_score(y[test_index], pred_labels))\n",
    "        accuracy_score\n",
    "        acc_list.append(accuracy_score(y[test_index], pred_labels))\n",
    "    return mean(f1_list), mean(acc_list)\n",
    "def train_optuna_func():\n",
    "    sampler = TPESampler(seed=10)  # Make the sampler behave in a deterministic way.\n",
    "    study = optuna.create_study( directions=[\"maximize\",\"maximize\"],sampler=sampler)\n",
    "    study.optimize(objective, n_trials=120)\n",
    "    trial =  max(study.best_trials, key=lambda i: i.values[0])\n",
    "    print(best)\n",
    "    print(\"  Value: {}\".format(trial.values))\n",
    "\n",
    "    print(\"  Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(\"    {}: {}\".format(key, value))\n",
    "\n",
    "def train_optuna_func_4search():\n",
    "    sampler = TPESampler(seed=10)  # Make the sampler behave in a deterministic way.\n",
    "    study = optuna.create_study( directions=[\"maximize\",\"maximize\"],sampler=sampler)\n",
    "    study.optimize(objective, n_trials=50)\n",
    "    return study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(deepcopy(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_list = []\n",
    "l = 5\n",
    "for i in range(3):\n",
    "    for j in range(2):\n",
    "        for k in [.8, .9, .99]:\n",
    "            print(\"ngram_range = (\", 1 + j,\";\", 3 + i,\") max_df = \", k,\" min_df\", l)\n",
    "            bow = CountVectorizer(min_df=l,max_df=k, ngram_range=(1 + j, 3 + i)) #remove rare and common words with df parameter\n",
    "            bow.fit(full_sent)\n",
    "            X = deepcopy(bow.transform(all_sent))\n",
    "            del bow\n",
    "            gc.collect()\n",
    "            study_list.append({\"ngram_range\":(1 + j, 3 + i),\"max_df\": k,\"min_df\": l, \"score\": train_optuna_func_4search()})\n",
    "best = max(study_list, key=lambda x: max(x['score'].best_trials, key=lambda i: i.values[0]))\n",
    "trial =  max(best['score'].best_trials, key=lambda i: i.values[0])\n",
    "print(best)\n",
    "# for i in best[\"score\"].best_trials:\n",
    "print(\"  Value: {}\".format(trial.values))\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TF-IDF\")\n",
    "study_list = []\n",
    "for i in range(3):\n",
    "    for j in range(2):\n",
    "        for k in [.8, .9, .99]:\n",
    "                #             for l in [5, 10, 25, 50, 100, 150]:\n",
    "                l = 5\n",
    "                print(\"ngram_range = (\", 1 + j,\";\", 3 + i,\") max_df = \", k,\" min_df\", l)\n",
    "                tfidf = TfidfVectorizer(min_df=l,max_df=k, ngram_range=(1 + j, 3 + i)) #remove rare and common words with df parameter\n",
    "                tfidf.fit(full_sent)\n",
    "                X = deepcopy(tfidf.transform(all_sent))\n",
    "                del tfidf\n",
    "                gc.collect()\n",
    "                study_list.append({\"ngram_range\":(1 + j, 3 + i),\"max_df\": k,\"min_df\": l, \"score\": train_optuna_func_4search()})\n",
    "best = max(study_list, key=lambda x: max(x['score'].best_trials, key=lambda i: i.values[0]))\n",
    "trial =  max(best['score'].best_trials, key=lambda i: i.values[0])\n",
    "print(best)\n",
    "print(\"  Value: {}\".format(trial.values))\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = gensim.downloader.load('word2vec-google-news-300') #1.66 gb\n",
    "X = np.array([vectorize_sentence(i, model=word2vec) for i in all_sent])\n",
    "del word2vec\n",
    "gc.collect()\n",
    "a = train_optuna_func()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gv = gensim.downloader.load('glove-wiki-gigaword-300') #376mb\n",
    "X = np.array([vectorize_sentence(i, model=gv) for i in all_sent])\n",
    "del gv\n",
    "gc.collect()\n",
    "train_optuna_func()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### USE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "universal = hub.load(\"./transformer_model\")\n",
    "X = np.vstack(np.array([universal([i]) for i in all_sent]))\n",
    "del universal\n",
    "gc.collect()\n",
    "train_optuna_func()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = SentenceTransformer('stsb-roberta-large') #1.3 gb\n",
    "X = np.array([bert.encode(i) for i in all_sent])\n",
    "del bert\n",
    "gc.collect()\n",
    "train_optuna_func()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MathBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = SentenceTransformer(\"tbs17/MathBERT\")\n",
    "X = np.array([bert.encode(i) for i in all_sent])\n",
    "del bert\n",
    "gc.collect()\n",
    "train_optuna_func()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
